{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "775dekHVI9gl"
      },
      "source": [
        "# ** Laboratorio 7: Implementación algoritmo de Newton**\n",
        "**Facultad de ingeniería, departamento de Ingeniería Biomédica, Universidad de los Andes**\\\n",
        "**IBIO-2440 Programación científica**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FHIOLOzzJK2L"
      },
      "source": [
        "**Nombres de los integrantes**\n",
        "\n",
        "\n",
        "1.   David Tobón Molina\n",
        "2.   David Santiago Rodríguez Quiroga\n",
        "\n",
        "**Número del grupo**\n",
        "\n",
        "Grupo 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO4ZxIlrwI1l"
      },
      "source": [
        "# **Método de Newton**\n",
        "El método de Newton es una alternativa al algoritmo de descenso de gradiente al tener en cuenta la segunda derivada de la función que se desea optimizar. Considere la función $f: \\mathbb{R}^n → \\mathbb{R}$. Sea $x[k]\\in\\mathbb{R}^n$ el valor del candidato a solución en la iteración $k$. La regla de actualización en el método de Newton es:\n",
        "\\begin{align*}\n",
        "  x[k+1] = x[k] - F^{-1}(x[k])∇f( x[k]),\n",
        "\\end{align*}\n",
        "donde $F^{-1}(x[k])$ es la inversa de la Hessiana evaluada en x[k]. \n",
        "\n",
        "\\\\\n",
        "\n",
        "El algoritmo completo se podría plantear de la siguiente manera:\n",
        "0. Definir k=0, un parámetro de convergengia $ϵ$ y un número máximo de iteraciones $N_{max}$\n",
        "1. Seleccionar un punto inicial $x[0]$\n",
        "2. Calcular $∇f( x[k])$ y $F^{-1}(x[k])$\n",
        "3. Calcular $x[k+1] = x[k] - F^{-1}(x[k])∇f( x[k])$\n",
        "4. Si $||x[k+1]-x[k]||_2<ϵ$, parar. Si no, $k=k+1$ y volver al paso 2.\n",
        "\n",
        "\n",
        "El objetivo de esta práctica es comparar el método de Newton con el algortimo de descenso de gradiente para minimizar una función de prueba dada con una condición de parada previamente definida. Para esto, considere la siguiente función:\n",
        "\n",
        "\\begin{align*}\n",
        " f(x) = (1-x_1)^2 + 5(x_2-x_1^2)^2,\n",
        "\\end{align*}\n",
        "\n",
        "donde $x=[x_1,x_2]^T$. Con base en lo anterior, siga los siguientes pasos:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFe88cV7-EVc"
      },
      "source": [
        "\n",
        "\n",
        "1. Encuentre el vector gradiente y la matriz Hessiana manualmente y escribalos a continuación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzzAsMzUfvrA"
      },
      "source": [
        "**Responder aquí**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q716aiOpfwZ0"
      },
      "source": [
        "2. Implemente el método de Newton teniendo en cuenta las condición de parada mencionada anteriormente con los siguientes parámetros:\n",
        "\n",
        " - $ϵ=0.1$\n",
        " - $x[0]=[0,0]^T$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0YdNLfBigZYy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x0 = np.array([0, 0]).T\n",
        "e = 0.1\n",
        "\n",
        "def func(x):\n",
        "    \n",
        "    return ((1 - x[0])**2) + (5 * ((x[1] - (x[0]**2))**2))\n",
        "\n",
        "\n",
        "def numeric_gradient(xi, f, h=0.000001):\n",
        "\n",
        "    gradient_result = np.zeros(len(xi))\n",
        "\n",
        "    for i in range(len(xi)):\n",
        "        xi_k = np.copy(xi)\n",
        "        xi_k[i] = xi[i]+h\n",
        "        gradient_result[i] = (f(xi_k)-f(xi))/h\n",
        "\n",
        "    return gradient_result\n",
        "\n",
        "\n",
        "def numeric_gradient2(xi):\n",
        "    \n",
        "    df_dx1 = (20*(xi[0]**3)) + (2*xi[0]) + (-20*xi[1]*xi[0]) - 2\n",
        "    df_dx2 = (10*xi[1]) + (-10*(xi[0]**2))\n",
        "    \n",
        "    return np.array([df_dx1, df_dx2])\n",
        "\n",
        "\n",
        "def hess(xi):\n",
        "    \n",
        "    df2_x1 = (60*(xi[0]**2)) + (-20*xi[1]) + 2\n",
        "    df2_x2 = 10\n",
        "    df2_x1_x2 = -20*xi[0]\n",
        "    \n",
        "    return np.array([[df2_x1, df2_x1_x2], [df2_x1_x2, df2_x2]])\n",
        "\n",
        "\n",
        "def metodo_de_Newton(x0, e, f=func, gradient_f=numeric_gradient2, hess_f=hess, n_max=500):\n",
        "    \n",
        "    sol_matrix = []\n",
        "    xk = np.copy(x0)\n",
        "    stop = False\n",
        "    n = 0\n",
        "    \n",
        "    while not stop:\n",
        "        \n",
        "        num_grad = gradient_f(xk)\n",
        "        #print(num_grad)\n",
        "        hessian = hess_f(xk)\n",
        "        #print(hessian)\n",
        "        inverse_hessian = np.linalg.inv(hessian)\n",
        "        #print(inverse_hessian)\n",
        "        \n",
        "        xk_1 = xk - np.dot(inverse_hessian, num_grad)\n",
        "        \n",
        "        norm = np.linalg.norm(xk_1 - xk)\n",
        "        \n",
        "        sol_matrix.append([])\n",
        "        sol_matrix[n].append(xk[0])\n",
        "        sol_matrix[n].append(xk[1])\n",
        "        sol_matrix[n].append(f(xk))\n",
        "        sol_matrix[n].append(norm)\n",
        "        sol_matrix[n].append(n+1)\n",
        "        \n",
        "        if norm < e:\n",
        "            stop = True\n",
        "        elif n+1 >= n_max:\n",
        "            stop = True\n",
        "        else:\n",
        "            xk = np.copy(xk_1)\n",
        "            n += 1\n",
        "    \n",
        "    return sol_matrix, [n+1, x0, xk, f(xk), e]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0, 0, 1, 1.0, 1], [1.0, 0.0, 5.0, 1.0, 2], [1.0, 1.0, 0.0, 0.0, 3]]\n",
            "\n",
            "\n",
            "[2, array([0, 0]), array([1., 1.]), 0.0, 0.1]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "resp = metodo_de_Newton(x0, e)\n",
        "\n",
        "for i in resp:\n",
        "    print(i)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKulyz2lgZqb"
      },
      "source": [
        "3. Realice tres gráficas:\n",
        "\n",
        "*   La trayectoria de $x[k]$ para los diferentes $k$ en un espacio de dos dimensiones. Es decir, los saltos que dió el algoritmo.\n",
        "*   El valor la función objetivo vs número de iteraciones $k$.\n",
        "*   El valor de $||x[k+1]-x[k]||_2$ vs número de iteraciones \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "fIv200w0kO41"
      },
      "outputs": [],
      "source": [
        "#Formato de sol matrix: [x1, x2, f(x), norma, n_it]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhUpvyJIkPPt"
      },
      "source": [
        "4. Ahora, use el algortimo de descenso de gradiente hecho en prácticas anteriores para encontrar el mínimo de la función. Use $α=0.01$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WnsbEdLk6NK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMjpOSxqk8RO"
      },
      "source": [
        "5. Para el algoritmo de descenso de gradiente implementado en el punto 4., realice tres gráficas:\n",
        "\n",
        "*   La trayectoria de $x[k]$ para los diferentes $k$ en un espacio de dos dimensiones. Es decir, los saltos que dió el algoritmo.\n",
        "*   El valor la función objetivo vs número de iteraciones $k$.\n",
        "*   El valor de $||x[k+1]-x[k]||_2$ vs número de iteraciones "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRwyBOOcnI_u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z1X8zw_nKPH"
      },
      "source": [
        "6. Responda las siguientes preguntas:\n",
        "\n",
        "- ¿Por qué cree que al algoritmo de descenso de gradiente le cuesta más converger que el método de Newton?\n",
        "\n",
        "- Note el grado del polinómio de la función, ¿para qué grado polinómico el método de Newton converge en una sola iteración?\n",
        "\n",
        "- ¿Cree que el punto inicial influye en la convergencia de ambos algortimos?\n",
        "\n",
        "- ¿Cómo podría mejorar la eficiencia del descenso de gradiente?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
